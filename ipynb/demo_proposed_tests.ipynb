{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will introduce you to `kcgof` (kernel conditional goodness-of-fit testing), a Python package implementing two kernel-based conditional goodness-of-fit tests as described in\n",
    "\n",
    "    Testing Goodness of Fit of Conditional Density Models with Kernels\n",
    "    Wittawat Jitkrittum, Heishiro Kanagawa, Bernhard Sch√∂lkopf\n",
    "    UAI 2020\n",
    "    https://arxiv.org/abs/2002.10271\n",
    "\n",
    "See [the Github page](https://github.com/wittawatj/kernel-cgof) for more information.\n",
    "\n",
    "Make sure that you have `kcgof` included in Python's search path. In particular the following import statements should not produce any fatal error. For how to install software dependencies, see the [README](https://github.com/wittawatj/kernel-cgof). In our code, we use [Pytorch](https://pytorch.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import kcgof\n",
    "import kcgof.log as klog\n",
    "import kcgof.util as util\n",
    "import kcgof.cdensity as cden\n",
    "import kcgof.cdata as cdat\n",
    "import kcgof.cgoftest as cgof\n",
    "import kcgof.kernel as ker\n",
    "import kcgof.plot as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# font options\n",
    "font = {\n",
    "    #'family' : 'normal',\n",
    "    #'weight' : 'bold',\n",
    "    'size'   : 20\n",
    "}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "plt.rc('lines', linewidth=2)\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Goodness-of-fit testing\n",
    "\n",
    "Given a known conditional density model $p(y|x)$ and a sample $\\{ (x_i, y_i) \\}_{i=1}^n \\sim q(y|x) r(x)$ where $q$ is the true conditional distribution generating the data, and $r$ is the marginal distribution for $x$, a conditional goodness-of-fit test proposes the following null hypothesis\n",
    "\n",
    "$H_0: p(y|x) = q(y|x) \\, \\text{for almost all}\\, x,y$ \n",
    "\n",
    "against the alternative hypothesis $H_1$ which is simply the negation of $H_0$.\n",
    "\n",
    "In other words, it tests whether or not the sample follows the model $p(y|x)$, for some marginal distribution $r$. Note that the model does not specify the marginal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our work\n",
    "\n",
    "We propose two new tests: \n",
    "\n",
    "1. The Kernel Conditional Stein Discrepancy (KCSD)\n",
    "2. The Finite Set Conditional Discrepancy (FSCD)\n",
    "\n",
    "The KCSD test makes a few mild assumptions on the distributions $p$ and $q$. \n",
    "The model $p(y|x)$ can take almost any form as long as it is differentiable. \n",
    "The normalizer of $p(y|x)$ (i.e., $p(x)$) is not assumed known. \n",
    "The test only assesses the goodness of the model through $\\nabla_{y} \\log p(y|x)$ i.e., the first derivative of the log conditional density. The FSCD is an extention of KCSD to return an interpretable set of locations in the space of $x$ that indicate where $p(y|x)$ differs from $q(y|x)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying your model\n",
    "\n",
    "In our code, a model is represented as an instance of `kcgof.cdensity.UnnormalizedCondModel`. You can subclass this class and implement your model. Alternatively, you can represent your model by the following convenient method that will construct an instance of this class for you.\n",
    "\n",
    "        kcgof.cdensity.from_log_den(dx, dy, f)\n",
    "        \n",
    "where:\n",
    "* `dx` is the dimension of x (positive integer)\n",
    "* `dy` is the dimension of y (positive integer)\n",
    "* `f` is a callable (a function) that takes two arguments (X, Y), where X: n x dx, Y: n x dy Torch tensors, and evaluates the unnormalized conditional density of the model i.e., p(x,y) (up to any mutiplicative factor as that is a function of x). Return a Torch array of length n.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary least squares with Gaussian noise\n",
    "\n",
    "\n",
    "For illustration, let us consider a simple one-dimensional ($d_x=1, d_y=1$) toy problem. Suppose our model is\n",
    "\n",
    "$$p(y|x) = \\mathcal{N}(y \\mid slope \\cdot x+c, variance),$$\n",
    "\n",
    "for some \"slope\", \"c\", and \"variance\". This is the standard least squares problem with Gaussian noise. \n",
    "In general, our tests can be applied to $x \\in \\mathbb{R}^{d_x}$ and $\\mathbb{R}^{d_y}$ where $d_x \\ge 1$ and $d_y \\ge 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, to specify the model, we first implement a function that computes the log density of the model, up to the normalizer. In this case, that would be \n",
    "\n",
    "$$ - \\frac{(y - (slope\\cdot x + c))^2}{2 \\times variance}.$$\n",
    "\n",
    "Note that the normalizer of the Gaussian density is dropped here. In general, any multiplicative factor on the density function that is independent of $y$ can be dropped. The reason is that our tests only use $\\nabla_y \\log p(y|x) = \\nabla_y \\log p(x, y)$. So any multiplicative factor will be irrelevant once the derivative $\\nabla_y$ is computed. The derivative will be computed automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define a function that implements the log density up to the normalizer\n",
    "def simple_gaussian_model(X, Y):    \n",
    "    # Some arbitrary model parameters here only for illustration.\n",
    "    # In practice, any models parameters are likely learned from \n",
    "    # an independent set of data before testing.\n",
    "    slope = torch.tensor([1.0])\n",
    "    c = 1.0\n",
    "    variance = 1.0\n",
    "    \n",
    "    # Note that X (n x dx), Y (n x dy) are Torch tensors\n",
    "    linear = torch.matmul(X, slope) + c\n",
    "    \n",
    "    # Note that this normalizing constant is not necessary. \n",
    "    # Try to set it to zero later to see it for yourself that the test result is not changed.\n",
    "    constant = -np.log(variance**0.5) - (2.0*np.pi)**0.5\n",
    "#     constant = 0\n",
    "    return -(Y.view(-1)-linear)**2/(2.0*variance) + constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = 1\n",
    "dy = 1\n",
    "# Model p is an instance of kcgof.cdensity.UnnormalizedCondModel \n",
    "# and is ready for conditional goodness-of-fit testing.\n",
    "p = cden.from_log_den(dx, dy, simple_gaussian_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A toy problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension of x. \n",
    "dx = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the test, let us generate some toy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.distributions as dists\n",
    "np.random.seed(10)\n",
    "real_slope = torch.tensor([1.3])\n",
    "real_variance = 1.0\n",
    "real_c = torch.tensor([1.0])\n",
    "\n",
    "# sample size\n",
    "n = 500 \n",
    "X = torch.tensor(np.random.randn(n, dx)*2.0, dtype=torch.float32)\n",
    "linear = torch.matmul(X, real_slope) + real_c\n",
    "output_noise = np.random.randn(n)*real_variance**0.5\n",
    "Y = linear + output_noise\n",
    "Y = Y.view(n, 1).type(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_data_model(p, X, Y, domX, domY, figsize=(10, 6), levels=20,\n",
    "    cmap='pink_r', **contourop ):\n",
    "    \"\"\"\n",
    "    Plot the conditional density model p(y|x) along with the data on a 2d plot.\n",
    "    Both x, and y must be scalar-valued. \n",
    "    p: cdensity.UnnormalizedCondDensity object representing the model p(y|x)\n",
    "    X, Y: n x 1 torch tensors for the data of x and y\n",
    "    domX: n x 1 torch tensor specifying points to plot in the domain of x\n",
    "    domY: n x 1 torch tensor specifying points to plot in the domain of y\n",
    "    \n",
    "    \"\"\"\n",
    "    mdomX, mdomY = torch.meshgrid(domX.view(-1), domY.view(-1))\n",
    "    flatlogden = p.log_den(mdomX.reshape(-1, 1), mdomY.reshape(-1, 1))\n",
    "    flatden = torch.exp(flatlogden)\n",
    "    # for the purpose of plotting, if the density is Nan, make it 0\n",
    "    flatden[torch.isnan(flatden)] = 0.0\n",
    "    mden = flatden.view(mdomX.shape)\n",
    "\n",
    "    np_mdomX = mdomX.detach().numpy()\n",
    "    np_mdomY = mdomY.detach().numpy()\n",
    "    np_mden = mden.detach().numpy()\n",
    "\n",
    "    plt.contourf(np_mdomX, np_mdomY, np_mden, levels=levels, cmap=cmap,\n",
    "        **contourop)\n",
    "    npX = X.detach().numpy()\n",
    "    npY = Y.detach().numpy()\n",
    "    plt.plot(npX, npY, 'bo', markersize=3)\n",
    "    plt.ylabel('$p(y|x)$')\n",
    "    # plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep = 0.7\n",
    "# make a grid that covers X\n",
    "domX = torch.linspace(torch.min(X)-ep, torch.max(X)+ep, 100)\n",
    "domY = torch.linspace(torch.min(Y).item()-ep, torch.max(Y).item()+ep, 200)\n",
    "\n",
    "plot_2d_data_model(p, X, Y, domX, domY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our model is slightly wrong since it has a wrong slope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Kernel Conditional Stein Discrepancy (KCSD) Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Gaussian kernels for both X and Y.\n",
    "# Let us use the median heuristic to pick the bandwidths.\n",
    "sigx = util.pt_meddistance(X, subsample=1000, seed=2)\n",
    "sigy = util.pt_meddistance(Y, subsample=1000, seed=3)\n",
    "        \n",
    "# k = kernel on X\n",
    "k = ker.PTKGauss(sigma2=sigx**2)\n",
    "\n",
    "# l = kernel on Y\n",
    "l = ker.PTKGauss(sigma2=sigy**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a KCSD test object\n",
    "# alpha = significant level for the test\n",
    "# n_bootstrap = the number of times to run wild bootstrap to simulate \n",
    "# the null distribution. The larger, the better.\n",
    "# The model p is used here to construct the test.\n",
    "kcsdtest = cgof.KCSDTest(p, k, l, alpha=0.05, n_bootstrap=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the actual test on the data \n",
    "result = kcsdtest.perform_test(X, Y)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the test rejects $H_0$ since the model is wrong. Go back and change the slope of the model to match the data generating process. With probability roughly $1-\\alpha$, the test will not reject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized KCSD test\n",
    "\n",
    "The KCSD test relies on two kernels $k$ and $l$ for $x$ and $y$ respectively. \n",
    "So far, we use Gaussian kernels and set the bandwidths by the median heuristic. \n",
    "A more principled way to choose the bandwidths is by optimizing them so as to \n",
    "maximize the asymptotic test power. If $H_1$ is true, optimizing the kernels\n",
    "in this way can lead to higher test power. If $H_0$ is true, the false rejection \n",
    "rate of $H_0$ is still under control (i.e., asymptotically no larger than $\\alpha$).\n",
    "\n",
    "For this particular toy problem that we consider, however, optimizing the two kernels\n",
    "may not be necessary since the problem is quite simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training set and test set.\n",
    "# The training set will be used to tune the two bandwidths.\n",
    "# Splitting ensures that the type-I error (false rejection rate)\n",
    "# is well controlled.\n",
    "\n",
    "# Proportion of the training set. The rest are for testing.\n",
    "tr_proportion = 0.3\n",
    "tr, te = cdat.CondData(X, Y).split_tr_te(tr_proportion=tr_proportion)\n",
    "\n",
    "# Xtr, Ytr are subset of the original data (X,Y)\n",
    "Xtr, Ytr = tr.xy()\n",
    "print(Xtr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytr.requires_grad = False\n",
    "kcsd_pc = cgof.KCSDPowerCriterion(p, k, l, Xtr, Ytr)\n",
    "\n",
    "max_iter = 200\n",
    "\n",
    "# learning rate \n",
    "lr = 1e-3\n",
    "# regularization\n",
    "reg = 1e-3\n",
    "\n",
    "# constraint satisfaction function\n",
    "def con_f(params):\n",
    "    ksigma2 = params[0]\n",
    "    lsigma2 = params[1]\n",
    "    ksigma2.data.clamp_(min=1e-2, max=10)\n",
    "    lsigma2.data.clamp_(min=1e-2, max=10)\n",
    "    \n",
    "objs = kcsd_pc.optimize_params(\n",
    "    [k.sigma2, l.sigma2], constraint_f=con_f,\n",
    "    lr=lr, reg=reg, max_iter=max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_objs = objs.detach().numpy()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(np.arange(max_iter), np_objs, 'b-')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('Power criterion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k.sigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.sigma2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a KCSD test object\n",
    "kcsdtest = cgof.KCSDTest(p, k, l, alpha=0.05, n_bootstrap=400, seed=9)\n",
    "Xte, Yte = te.xy()\n",
    "kcsdtest.perform_test(Xte, Yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
